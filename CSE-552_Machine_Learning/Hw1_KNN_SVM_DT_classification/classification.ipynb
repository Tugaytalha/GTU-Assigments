{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment - KNN, SVM and DT Experiments\n",
    "\n",
    "This notebook implements experiments with KNN, SVM, and Decision Trees for both classification (Breast Cancer Wisconsin Diagnostic) and regression (Bike Sharing) problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_curve, auc\n",
    "from sklearn import svm, tree\n",
    "import time\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer Wisconsin Dataset\n",
    "breast_cancer_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "response = requests.get(breast_cancer_url)\n",
    "data = StringIO(response.text)\n",
    "\n",
    "# Define column names\n",
    "# ID, diagnosis (M = malignant, B = benign), and 30 feature columns\n",
    "column_names = ['id', 'diagnosis'] + [f'feature_{i}' for i in range(1, 31)]\n",
    "breast_cancer_df = pd.read_csv(data, header=None, names=column_names)\n",
    "\n",
    "# Convert diagnosis to binary (1 for Malignant, 0 for Benign)\n",
    "breast_cancer_df['diagnosis'] = breast_cancer_df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Drop ID column\n",
    "breast_cancer_df = breast_cancer_df.drop('id', axis=1)\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"Breast Cancer Dataset Preview:\")\n",
    "breast_cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bike Sharing Dataset\n",
    "bike_sharing_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\"\n",
    "\n",
    "# For simplicity, let's download locally and read as pandas\n",
    "# In Colab, you could use:\n",
    "# !wget -O bike-sharing.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
    "# !unzip -o bike-sharing.zip\n",
    "# bike_sharing_df = pd.read_csv('day.csv')\n",
    "\n",
    "# For this notebook, we'll use a direct approach:\n",
    "import zipfile\n",
    "import io\n",
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen(bike_sharing_url) as response:\n",
    "    with zipfile.ZipFile(io.BytesIO(response.read())) as zip_ref:\n",
    "        with zip_ref.open('day.csv') as f:\n",
    "            bike_sharing_df = pd.read_csv(f)\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"Bike Sharing Dataset Preview:\")\n",
    "bike_sharing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Breast Cancer Dataset\n",
    "X_cancer = breast_cancer_df.drop('diagnosis', axis=1)\n",
    "y_cancer = breast_cancer_df['diagnosis']\n",
    "\n",
    "# Scale features\n",
    "scaler_cancer = StandardScaler()\n",
    "X_cancer_scaled = scaler_cancer.fit_transform(X_cancer)\n",
    "\n",
    "# Preprocess Bike Sharing Dataset\n",
    "# Remove instant, dteday and casual/registered (as cnt is their sum)\n",
    "bike_sharing_df = bike_sharing_df.drop(['instant', 'dteday', 'casual', 'registered'], axis=1)\n",
    "\n",
    "# Define features and target\n",
    "X_bike = bike_sharing_df.drop('cnt', axis=1)\n",
    "y_bike = bike_sharing_df['cnt']\n",
    "\n",
    "# Scale features\n",
    "scaler_bike = StandardScaler()\n",
    "X_bike_scaled = scaler_bike.fit_transform(X_bike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: KNN Classifier with Euclidean Distance (K=3)\n",
    "\n",
    "In this part, we implement a KNN classifier with Euclidean distance and test it on the Breast Cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "    \n",
    "    def manhattan_distance(self, x1, x2):\n",
    "        return np.sum(np.abs(x1 - x2))\n",
    "    \n",
    "    def get_distance(self, x1, x2):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return self.euclidean_distance(x1, x2)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return self.manhattan_distance(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        # Calculate distances between x and all examples in the training set\n",
    "        distances = [self.get_distance(x, x_train) for x_train in self.X_train]\n",
    "        \n",
    "        # Get k nearest samples, labels\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        \n",
    "        # Return most common class\n",
    "        most_common = np.bincount(k_nearest_labels).argmax()\n",
    "        return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn_classifier(X, y, k=3, distance_metric='euclidean', n_folds=6):\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    accuracies = []\n",
    "    conf_matrices = []\n",
    "    times = []\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train classifier\n",
    "        start_time = time.time()\n",
    "        clf = KNNClassifier(k=k, distance_metric=distance_metric)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = clf.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Store metrics\n",
    "        accuracies.append(accuracy)\n",
    "        conf_matrices.append(conf_matrix)\n",
    "        times.append(execution_time)\n",
    "        \n",
    "        # Print metrics for the first fold (as required)\n",
    "        if fold == 0:\n",
    "            print(f\"Fold 1 results:\")\n",
    "            print(f\"Training/test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Confusion Matrix:\")\n",
    "            print(conf_matrix)\n",
    "            print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "    \n",
    "    # Print average metrics\n",
    "    print(\"\\nAverage results across all folds:\")\n",
    "    print(f\"Average Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "    print(f\"Average Execution time: {np.mean(times):.4f} ± {np.std(times):.4f} seconds\")\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'accuracies': accuracies,\n",
    "        'conf_matrices': conf_matrices,\n",
    "        'times': times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KNN Classifier with Euclidean distance\n",
    "print(\"KNN Classifier with Euclidean Distance (K=3) on Breast Cancer Dataset:\")\n",
    "knn_classifier_results = evaluate_knn_classifier(\n",
    "    X_cancer_scaled, \n",
    "    y_cancer.values, \n",
    "    k=3, \n",
    "    distance_metric='euclidean',\n",
    "    n_folds=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results Summary\n",
    "\n",
    "The custom-implemented KNN classifier with Euclidean distance (K=3) has been evaluated on the Breast Cancer Wisconsin dataset using 6-fold cross-validation. The results show the classifier's performance in terms of accuracy, confusion matrices, and execution time.\n",
    "\n",
    "The confusion matrix interpretation:\n",
    "- True Negatives (TN): Correctly predicted benign samples\n",
    "- False Positives (FP): Benign samples incorrectly predicted as malignant\n",
    "- False Negatives (FN): Malignant samples incorrectly predicted as benign\n",
    "- True Positives (TP): Correctly predicted malignant samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: KNN Regressor with Manhattan Distance (K=3)\n",
    "\n",
    "In this part, we implement a KNN regressor with Manhattan distance and test it on the Bike Sharing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRegressor:\n",
    "    def __init__(self, k=3, distance_metric='manhattan'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "    \n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "    \n",
    "    def manhattan_distance(self, x1, x2):\n",
    "        return np.sum(np.abs(x1 - x2))\n",
    "    \n",
    "    def get_distance(self, x1, x2):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return self.euclidean_distance(x1, x2)\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            return self.manhattan_distance(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {self.distance_metric}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        # Calculate distances between x and all examples in the training set\n",
    "        distances = [self.get_distance(x, x_train) for x_train in self.X_train]\n",
    "        \n",
    "        # Get k nearest samples, labels\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_targets = [self.y_train[i] for i in k_indices]\n",
    "        \n",
    "        # Return mean of k nearest neighbors\n",
    "        return np.mean(k_nearest_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn_regressor(X, y, k=3, distance_metric='manhattan', n_folds=6):\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    mse_scores = []\n",
    "    r2_scores = []\n",
    "    times = []\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train regressor\n",
    "        start_time = time.time()\n",
    "        reg = KNNRegressor(k=k, distance_metric=distance_metric)\n",
    "        reg.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = reg.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Store metrics\n",
    "        mse_scores.append(mse)\n",
    "        r2_scores.append(r2)\n",
    "        times.append(execution_time)\n",
    "        \n",
    "        # Print metrics for the first fold (as required)\n",
    "        if fold == 0:\n",
    "            print(f\"Fold 1 results:\")\n",
    "            print(f\"Training/test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "            print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "            print(f\"R² Score: {r2:.4f}\")\n",
    "            print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "            \n",
    "            # Plot actual vs predicted for the first fold\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "            plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "            plt.xlabel('Actual')\n",
    "            plt.ylabel('Predicted')\n",
    "            plt.title('KNN Regressor: Actual vs Predicted (Fold 1)')\n",
    "            plt.show()\n",
    "    \n",
    "    # Print average metrics\n",
    "    print(\"\\nAverage results across all folds:\")\n",
    "    print(f\"Average MSE: {np.mean(mse_scores):.4f} ± {np.std(mse_scores):.4f}\")\n",
    "    print(f\"Average R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "    print(f\"Average Execution time: {np.mean(times):.4f} ± {np.std(times):.4f} seconds\")\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'mse_scores': mse_scores,\n",
    "        'r2_scores': r2_scores,\n",
    "        'times': times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KNN Regressor with Manhattan distance\n",
    "print(\"KNN Regressor with Manhattan Distance (K=3) on Bike Sharing Dataset:\")\n",
    "knn_regressor_results = evaluate_knn_regressor(\n",
    "    X_bike_scaled, \n",
    "    y_bike.values, \n",
    "    k=3, \n",
    "    distance_metric='manhattan',\n",
    "    n_folds=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Results Summary\n",
    "\n",
    "The custom-implemented KNN regressor with Manhattan distance (K=3) has been evaluated on the Bike Sharing dataset using 6-fold cross-validation. The results show the regressor's performance in terms of Mean Squared Error (MSE), R² score, and execution time.\n",
    "\n",
    "The scatter plot shows the relationship between actual and predicted values for the first fold, with the red dashed line representing perfect predictions. This helps visualize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Linear SVM Classifier\n",
    "\n",
    "In this part, we implement a classifier based on linear SVM and test it on the Breast Cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_svm_classifier(X, y, n_folds=6):\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    accuracies = []\n",
    "    conf_matrices = []\n",
    "    times = []\n",
    "    roc_aucs = []\n",
    "    tprs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    # For plotting ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train classifier\n",
    "        start_time = time.time()\n",
    "        clf = svm.SVC(kernel='linear', probability=True, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Get decision scores (distance from hyperplane)\n",
    "        y_scores = clf.decision_function(X_test)\n",
    "        \n",
    "        # Get probabilities for ROC curve\n",
    "        y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Predict with default threshold\n",
    "        y_pred = clf.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Calculate ROC curve and area\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_aucs.append(roc_auc)\n",
    "        \n",
    "        # Interpolate TPR values for mean curve calculation\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        \n",
    "        # Find the optimal threshold using Youden's J statistic (max(TPR - FPR))\n",
    "        j_scores = tpr - fpr\n",
    "        optimal_idx = np.argmax(j_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_thresholds.append(optimal_threshold)\n",
    "        \n",
    "        # Apply optimal threshold to get predictions\n",
    "        y_pred_optimal = (y_probs >= optimal_threshold).astype(int)\n",
    "        optimal_acc = accuracy_score(y_test, y_pred_optimal)\n",
    "        optimal_conf_matrix = confusion_matrix(y_test, y_pred_optimal)\n",
    "        \n",
    "        # Plot ROC curve for each fold\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "                 label=f'ROC fold {fold+1} (AUC = {roc_auc:.2f})')\n",
    "        \n",
    "        # Store metrics\n",
    "        accuracies.append(optimal_acc)  # Using accuracy with optimal threshold\n",
    "        conf_matrices.append(optimal_conf_matrix)  # Using confusion matrix with optimal threshold\n",
    "        times.append(execution_time)\n",
    "        \n",
    "        # Print metrics for the first fold (as required)\n",
    "        if fold == 0:\n",
    "            print(f\"Fold 1 results:\")\n",
    "            print(f\"Training/test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "            print(f\"Default Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Default Confusion Matrix:\")\n",
    "            print(conf_matrix)\n",
    "            print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
    "            print(f\"Optimal Accuracy: {optimal_acc:.4f}\")\n",
    "            print(f\"Optimal Confusion Matrix:\")\n",
    "            print(optimal_conf_matrix)\n",
    "            print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "            print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "    \n",
    "    # Plot mean ROC curve\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(roc_aucs)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'b-',\n",
    "             label=f'Mean ROC (AUC = {mean_auc:.2f} ± {std_auc:.2f})',\n",
    "             lw=2, alpha=0.8)\n",
    "\n",
    "    # Plot standard deviation\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.2,\n",
    "                      label=f'± 1 std. dev.')\n",
    "    \n",
    "    # Plot diagonal\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    \n",
    "    # Set plot properties\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Linear SVM Classifier')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print average metrics\n",
    "    print(\"\\nAverage results across all folds:\")\n",
    "    print(f\"Average Accuracy (with optimal thresholds): {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "    print(f\"Average ROC AUC: {np.mean(roc_aucs):.4f} ± {np.std(roc_aucs):.4f}\")\n",
    "    print(f\"Average Optimal Threshold: {np.mean(optimal_thresholds):.4f} ± {np.std(optimal_thresholds):.4f}\")\n",
    "    print(f\"Average Execution time: {np.mean(times):.4f} ± {np.std(times):.4f} seconds\")\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'accuracies': accuracies,\n",
    "        'conf_matrices': conf_matrices,\n",
    "        'roc_aucs': roc_aucs,\n",
    "        'optimal_thresholds': optimal_thresholds,\n",
    "        'times': times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear SVM Classifier\n",
    "print(\"Linear SVM Classifier on Breast Cancer Dataset:\")\n",
    "svm_classifier_results = evaluate_svm_classifier(\n",
    "    X_cancer_scaled, \n",
    "    y_cancer.values,\n",
    "    n_folds=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Results Summary\n",
    "\n",
    "The linear SVM classifier has been evaluated on the Breast Cancer Wisconsin dataset using 6-fold cross-validation. The results show the classifier's performance in terms of accuracy, confusion matrices, ROC curves, and execution time.\n",
    "\n",
    "As required, we found the best threshold for the SVM output using Youden's J statistic, which maximizes the difference between the true positive rate and false positive rate. This optimal threshold was used to generate the final predictions and confusion matrices.\n",
    "\n",
    "The ROC curve demonstrates the trade-off between sensitivity (true positive rate) and specificity (1 - false positive rate) at various threshold settings. The area under the ROC curve (AUC) provides an aggregate measure of performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Linear SVM Regressor\n",
    "\n",
    "In this part, we implement a regressor based on linear SVM and test it on the Bike Sharing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_svm_regressor(X, y, n_folds=6):\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    mse_scores = []\n",
    "    r2_scores = []\n",
    "    times = []\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train regressor\n",
    "        start_time = time.time()\n",
    "        reg = svm.SVR(kernel='linear')\n",
    "        reg.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = reg.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Store metrics\n",
    "        mse_scores.append(mse)\n",
    "        r2_scores.append(r2)\n",
    "        times.append(execution_time)\n",
    "        \n",
    "        # Print metrics for the first fold (as required)\n",
    "        if fold == 0:\n",
    "            print(f\"Fold 1 results:\")\n",
    "            print(f\"Training/test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "            print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "            print(f\"R² Score: {r2:.4f}\")\n",
    "            print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "            \n",
    "            # Plot actual vs predicted for the first fold\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "            plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "            plt.xlabel('Actual')\n",
    "            plt.ylabel('Predicted')\n",
    "            plt.title('SVM Regressor: Actual vs Predicted (Fold 1)')\n",
    "            plt.show()\n",
    "    \n",
    "    # Print average metrics\n",
    "    print(\"\\nAverage results across all folds:\")\n",
    "    print(f\"Average MSE: {np.mean(mse_scores):.4f} ± {np.std(mse_scores):.4f}\")\n",
    "    print(f\"Average R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "    print(f\"Average Execution time: {np.mean(times):.4f} ± {np.std(times):.4f} seconds\")\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'mse_scores': mse_scores,\n",
    "        'r2_scores': r2_scores,\n",
    "        'times': times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear SVM Regressor\n",
    "print(\"Linear SVM Regressor on Bike Sharing Dataset:\")\n",
    "svm_regressor_results = evaluate_svm_regressor(\n",
    "    X_bike_scaled, \n",
    "    y_bike.values,\n",
    "    n_folds=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Results Summary\n",
    "\n",
    "The linear SVM regressor has been evaluated on the Bike Sharing dataset using 6-fold cross-validation. The results show the regressor's performance in terms of Mean Squared Error (MSE), R² score, and execution time.\n",
    "\n",
    "The scatter plot shows the relationship between actual and predicted values for the first fold, with the red dashed line representing perfect predictions. This helps visualize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Decision Tree Classifier\n",
    "\n",
    "In this part, we implement a classifier based on Decision Trees and test it on the Breast Cancer dataset.\n",
    "We'll experiment with two different pruning strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rules(tree, feature_names, class_names):\n",
    "    \"\"\"Extract rules from a decision tree\"\"\"\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != tree_.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    \n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != tree_.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            \n",
    "            # Add left node (<=)\n",
    "            path.append((name, \"<=\", threshold))\n",
    "            recurse(tree_.children_left[node], path, paths)\n",
    "            \n",
    "            # Remove last item for the right branch\n",
    "            path.pop()\n",
    "            \n",
    "            # Add right node (>)\n",
    "            path.append((name, \">\", threshold))\n",
    "            recurse(tree_.children_right[node], path, paths)\n",
    "            \n",
    "            # Remove last item after both branches are processed\n",
    "            path.pop()\n",
    "        else:\n",
    "            # Leaf node - get the class that has the majority\n",
    "            class_idx = np.argmax(tree_.value[node])\n",
    "            class_name = class_names[class_idx]\n",
    "            paths.append((path.copy(), class_name))\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "    \n",
    "    # Convert to rules\n",
    "    rules = []\n",
    "    for path, class_name in paths:\n",
    "        rule = \"IF \" + \" AND \".join([f\"{name} {inequality} {threshold:.4f}\" for name, inequality, threshold in path])\n",
    "        rule += f\" THEN class = {class_name}\"\n",
    "        rules.append(rule)\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dt_classifier(X, y, feature_names, class_names=[0, 1], n_folds=6):\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize metrics storage for different pruning strategies\n",
    "    results = {\n",
    "        'no_pruning': {\n",
    "            'accuracies': [],\n",
    "            'times': [],\n",
    "            'model': None\n",
    "        },\n",
    "        'max_depth': {\n",
    "            'accuracies': [],\n",
    "            'times': [],\n",
    "            'model': None\n",
    "        },\n",
    "        'min_samples_leaf': {\n",
    "            'accuracies': [],\n",
    "            'times': [],\n",
    "            'model': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train with no pruning\n",
    "        start_time = time.time()\n",
    "        dt_no_pruning = tree.DecisionTreeClassifier(random_state=42)\n",
    "        dt_no_pruning.fit(X_train, y_train)\n",
    "        y_pred_no_pruning = dt_no_pruning.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        acc_no_pruning = accuracy_score(y_test, y_pred_no_pruning)\n",
    "        time_no_pruning = end_time - start_time\n",
    "        \n",
    "        # Train with max_depth pruning\n",
    "        start_time = time.time()\n",
    "        dt_max_depth = tree.DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "        dt_max_depth.fit(X_train, y_train)\n",
    "        y_pred_max_depth = dt_max_depth.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        acc_max_depth = accuracy_score(y_test, y_pred_max_depth)\n",
    "        time_max_depth = end_time - start_time\n",
    "        \n",
    "        # Train with min_samples_leaf pruning\n",
    "        start_time = time.time()\n",
    "        dt_min_samples = tree.DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\n",
    "        dt_min_samples.fit(X_train, y_train)\n",
    "        y_pred_min_samples = dt_min_samples.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        acc_min_samples = accuracy_score(y_test, y_pred_min_samples)\n",
    "        time_min_samples = end_time - start_time\n",
    "        \n",
    "        # Store metrics\n",
    "        results['no_pruning']['accuracies'].append(acc_no_pruning)\n",
    "        results['no_pruning']['times'].append(time_no_pruning)\n",
    "        \n",
    "        results['max_depth']['accuracies'].append(acc_max_depth)\n",
    "        results['max_depth']['times'].append(time_max_depth)\n",
    "        \n",
    "        results['min_samples_leaf']['accuracies'].append(acc_min_samples)\n",
    "        results['min_samples_leaf']['times'].append(time_min_samples)\n",
    "        \n",
    "        # Store models for first fold\n",
    "        if fold == 0:\n",
    "            results['no_pruning']['model'] = dt_no_pruning\n",
    "            results['max_depth']['model'] = dt_max_depth\n",
    "            results['min_samples_leaf']['model'] = dt_min_samples\n",
    "            \n",
    "            print(f\"Fold 1 results:\")\n",
    "            print(f\"Training/test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "            print(\"\\nNo Pruning:\")\n",
    "            print(f\"Tree size (number of nodes): {dt_no_pruning.tree_.node_count}\")\n",
    "            print(f\"Maximum depth: {dt_no_pruning.tree_.max_depth}\")\n",
    "            print(f\"Accuracy: {acc_no_pruning:.4f}\")\n",
    "            print(f\"Execution time: {time_no_pruning:.4f} seconds\")\n",
    "            \n",
    "            print(\"\\nMax Depth Pruning (max_depth=5):\")\n",
    "            print(f\"Tree size (number of nodes): {dt_max_depth.tree_.node_count}\")\n",
    "            print(f\"Maximum depth: {dt_max_depth.tree_.max_depth}\")\n",
    "            print(f\"Accuracy: {acc_max_depth:.4f}\")\n",
    "            print(f\"Execution time: {time_max_depth:.4f} seconds\")\n",
    "            \n",
    "            print(\"\\nMin Samples Leaf Pruning (min_samples_leaf=5):\")\n",
    "            print(f\"Tree size (number of nodes): {dt_min_samples.tree_.node_count}\")\n",
    "            print(f\"Maximum depth: {dt_min_samples.tree_.max_depth}\")\n",
    "            print(f\"Accuracy: {acc_min_samples:.4f}\")\n",
    "            print(f\"Execution time: {time_min_samples:.4f} seconds\")\n",
    "    \n",
    "    # Print average metrics\n",
    "    print(\"\\nAverage results across all folds:\")\n",
    "    print(f\"No Pruning - Average Accuracy: {np.mean(results['no_pruning']['accuracies']):.4f} ± {np.std(results['no_pruning']['accuracies']):.4f}\")\n",
    "    print(f\"Max Depth Pruning - Average Accuracy: {np.mean(results['max_depth']['accuracies']):.4f} ± {np.std(results['max_depth']['accuracies']):.4f}\")\n",
    "    print(f\"Min Samples Leaf Pruning - Average Accuracy: {np.mean(results['min_samples_leaf']['accuracies']):.4f} ± {np.std(results['min_samples_leaf']['accuracies']):.4f}\")\n",
    "    \n",
    "    # Extract rules from the best pruning strategy model of the first fold\n",
    "    best_strategy = max(['no_pruning', 'max_depth', 'min_samples_leaf'], \n",
    "                        key=lambda x: np.mean(results[x]['accuracies']))\n",
    "    \n",
    "    print(f\"\\nExtracting rules from the best model: {best_strategy}\")\n",
    "    best_model = results[best_strategy]['model']\n",
    "    rules = extract_rules(best_model, feature_names, class_names)\n",
    "    \n",
    "    print(f\"Number of rules: {len(rules)}\")\n",
    "    print(\"Sample rules (first 5):\")\n",
    "    for i, rule in enumerate(rules[:5]):\n",
    "        print(f\"Rule {i+1}: {rule}\")\n",
    "    \n",
    "    # Visualize the best tree if it's small enough\n",
    "    if best_model.tree_.node_count < 50:  # Only visualize if tree is not too large\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        tree.plot_tree(best_model, feature_names=feature_names, class_names=[str(c) for c in class_names], filled=True)\n",
    "        plt.title(f\"Decision Tree Visualization ({best_strategy})\")\n",
    "        plt.show()\n",
    "    \n",
    "    return results, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feature names for the breast cancer dataset\n",
    "cancer_feature_names = X_cancer.columns.tolist()\n",
    "\n",
    "# Run Decision Tree Classifier\n",
    "print(\"Decision Tree Classifier on Breast Cancer Dataset:\")\n",
    "dt_classifier_results, dt_rules = evaluate_dt_classifier(\n",
    "    X_cancer_scaled, \n",
    "    y_cancer.values,\n",
    "    feature_names=cancer_feature_names,\n",
    "    class_names=['Benign (0)', 'Malignant (1)'],\n",
    "    n_folds=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 Results Summary\n",
    "\n",
    "The Decision Tree classifier has been evaluated on the Breast Cancer Wisconsin dataset using 6-fold cross-validation. We experimented with two different pruning strategies:\n",
    "\n",
    "1. **Max Depth Pruning**: This strategy limits the maximum depth of the tree to 5 levels, preventing the tree from growing too deep and potentially overfitting. This reduces the complexity of the model and can improve generalization.\n",
    "\n",
    "2. **Min Samples Leaf Pruning**: This strategy requires each leaf node to have at least 5 samples, which prevents the creation of very specific rules based on just a few training examples. This also helps prevent overfitting.\n",
    "\n",
    "We also included a baseline model with no pruning to compare the effectiveness of the pruning strategies.\n",
    "\n",
    "Additionally, we've extracted decision rules from the best-performing model. These rules provide a human-readable interpretation of the model's decision-making process, making it easier to understand how the model classifies samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Decision Tree Regressor\n",
    "\n",
    "In this part, we implement a regressor based on Decision Trees and test it on the Bike Sharing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_regression_rules(tree, feature_names):\n",
    "    \"\"\"Extract rules from a regression tree\"\"\"\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != tree_.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    \n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != tree_.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            \n",
    "            # Add left node (<=)\n",
    "            path.append((name, \"<=\", threshold))\n",
    "            recurse(tree_.children_left[node], path, paths)\n",
    "            \n",
    "            # Remove last item for the right branch\n",
    "            path.pop()\n",
    "            \n",
    "            # Add right node (>)\n",
    "            path.append((name, \">\", threshold))\n",
    "            recurse(tree_.children_right[node], path, paths)\n",
    "            \n",
    "            # Remove last item after both branches are processed\n",
    "            path.pop()\n",
    "        else:\n",
    "            # Leaf node - get the prediction value\n",
    "            value = tree_.value[node][0][0]  # Mean value of samples in this leaf\n",
    "            paths.append((path.copy(), value))\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "    \n",
    "    # Convert to rules\n",
    "    rules = []\n",
    "    for path, value in paths:\n",
    "        rule = \"IF \" + \" AND \".join([f\"{name} {inequality} {threshold:.4f}\" for name, inequality, threshold in path])\n",
    "        rule += f\" THEN value = {value:.4f}\"\n",
    "        rules.append(rule)\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dt_regressor(X, y, feature_names, n_folds=6):\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    mse_scores = []\n",
    "    r2_scores = []\n",
    "    times = []\n",
    "    best_model = None\n",
    "    best_fold_r2 = -float('inf')\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train regressor\n",
    "        start_time = time.time()\n",
    "        dt = tree.DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "        dt.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = dt.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Store metrics\n",
    "        mse_scores.append(mse)\n",
    "        r2_scores.append(r2)\n",
    "        times.append(execution_time)\n",
    "        \n",
    "        # Keep track of the best model\n",
    "        if r2 > best_fold_r2:\n",
    "            best_fold_r2 = r2\n",
    "            best_model = dt\n",
    "        \n",
    "        # Print metrics for the first fold (as required)\n",
    "        if fold == 0:\n",
    "            print(f\"Fold 1 results:\")\n",
    "            print(f\"Training/test split: {len(X_train)}/{len(X_test)} samples\")\n",
    "            print(f\"Tree size (number of nodes): {dt.tree_.node_count}\")\n",
    "            print(f\"Maximum depth: {dt.tree_.max_depth}\")\n",
    "            print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "            print(f\"R² Score: {r2:.4f}\")\n",
    "            print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "            \n",
    "            # Plot actual vs predicted for the first fold\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "            plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "            plt.xlabel('Actual')\n",
    "            plt.ylabel('Predicted')\n",
    "            plt.title('Decision Tree Regressor: Actual vs Predicted (Fold 1)')\n",
    "            plt.show()\n",
    "    \n",
    "    # Print average metrics\n",
    "    print(\"\\nAverage results across all folds:\")\n",
    "    print(f\"Average MSE: {np.mean(mse_scores):.4f} ± {np.std(mse_scores):.4f}\")\n",
    "    print(f\"Average R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "    print(f\"Average Execution time: {np.mean(times):.4f} ± {np.std(times):.4f} seconds\")\n",
    "    \n",
    "    # Extract rules from the best model\n",
    "    print(f\"\\nExtracting rules from the best model (fold with highest R²)\")\n",
    "    rules = extract_regression_rules(best_model, feature_names)\n",
    "    \n",
    "    print(f\"Number of rules: {len(rules)}\")\n",
    "    print(\"Sample rules (first 5):\")\n",
    "    for i, rule in enumerate(rules[:5]):\n",
    "        print(f\"Rule {i+1}: {rule}\")\n",
    "    \n",
    "    # Visualize tree if it's small enough\n",
    "    if best_model.tree_.node_count < 50:  # Only visualize if tree is not too large\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        tree.plot_tree(best_model, feature_names=feature_names, filled=True)\n",
    "        plt.title(\"Decision Tree Regression Visualization\")\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'mse_scores': mse_scores,\n",
    "        'r2_scores': r2_scores,\n",
    "        'times': times,\n",
    "        'rules': rules,\n",
    "        'best_model': best_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feature names for the bike sharing dataset\n",
    "bike_feature_names = X_bike.columns.tolist()\n",
    "\n",
    "# Run Decision Tree Regressor\n",
    "print(\"Decision Tree Regressor on Bike Sharing Dataset:\")\n",
    "dt_regressor_results = evaluate_dt_regressor(\n",
    "    X_bike_scaled, \n",
    "    y_bike.values,\n",
    "    feature_names=bike_feature_names,\n",
    "    n_folds=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6 Results Summary\n",
    "\n",
    "The Decision Tree regressor has been evaluated on the Bike Sharing dataset using 6-fold cross-validation. We've configured the model with a maximum depth of 10 to prevent overfitting while still allowing it to capture the underlying patterns in the data.\n",
    "\n",
    "The model's performance has been assessed using Mean Squared Error (MSE) and R² score metrics, which provide insights into both the absolute prediction errors and the proportion of variance explained by the model.\n",
    "\n",
    "We've also extracted regression rules from the best-performing model (the one with the highest R² score). These rules provide a transparent representation of how the model makes predictions, showing the conditions that lead to different bike rental count predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this assignment, we've implemented and evaluated various machine learning algorithms for classification and regression tasks:\n",
    "\n",
    "1. **KNN Classifier with Euclidean distance**: We implemented a custom KNN classifier and evaluated it on the Breast Cancer dataset. The model demonstrated its ability to effectively classify tumors as benign or malignant.\n",
    "\n",
    "2. **KNN Regressor with Manhattan distance**: Our custom KNN regressor was tested on the Bike Sharing dataset, showing how the k-nearest neighbors approach can be applied to regression problems.\n",
    "\n",
    "3. **Linear SVM Classifier**: We utilized scikit-learn's SVM implementation for classification on the Breast Cancer dataset, examining the model's performance using ROC curves and finding optimal decision thresholds.\n",
    "\n",
    "4. **Linear SVM Regressor**: We applied SVR to the Bike Sharing dataset, demonstrating how support vector methods can be extended to regression problems.\n",
    "\n",
    "5. **Decision Tree Classifier**: We experimented with different pruning strategies (max depth and min samples leaf) to understand their impact on model complexity and performance for the Breast Cancer dataset.\n",
    "\n",
    "6. **Decision Tree Regressor**: We applied decision trees to the regression task with the Bike Sharing dataset and extracted human-readable rules from the model.\n",
    "\n",
    "For each model, we reported performance metrics, execution times, and visual representations of the results. We also implemented rule extraction for decision tree models, providing interpretable insights into the models' decision-making processes.\n",
    "\n",
    "This comprehensive evaluation provides a solid foundation for understanding the strengths and weaknesses of each algorithm in different contexts, as well as the impact of various parameter choices and pruning strategies on model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
